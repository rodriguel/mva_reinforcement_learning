{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LELOTTE_MVARL19_part1.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y-dRm0X_3ZM",
        "colab_type": "text"
      },
      "source": [
        "# Notes\n",
        "/!\\ This notebook is fully functional on Google Colab./!\\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4P3WM-hVOPfo"
      },
      "source": [
        "# Reinforcement Learning in Finite MDPs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E9_DLZvWQzhb",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/rlgammazero/mvarl_hands_on.git > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tzgegvml_KUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wnzUJeyJOPfq"
      },
      "source": [
        "## MDPs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RcWJSw_uOPfr",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, './mvarl_hands_on/utils')\n",
        "import numpy as np\n",
        "from scipy.special import softmax # for SARSA\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import math\n",
        "from cliffwalk import CliffWalk\n",
        "from test_env import ToyEnv1\n",
        "import gridworld"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ym-B_4HaOPfu"
      },
      "source": [
        "Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rVR5qYoLOPfv",
        "colab": {}
      },
      "source": [
        "env = CliffWalk(proba_succ=0.98)\n",
        "\n",
        "####################################################################################\n",
        "# You probably want to test smaller enviroments before\n",
        "#env = ToyEnv1(gamma=0.99)\n",
        "####################################################################################\n",
        "\n",
        "# Useful attributes\n",
        "print(\"Set of states:\", env.states)\n",
        "print(\"Set of actions:\", env.actions)\n",
        "print(\"Number of states: \", env.Ns)\n",
        "print(\"Number of actions: \", env.Na)\n",
        "print(\"P has shape: \", env.P.shape)  # P[s, a, s'] = env.P[s, a, s']\n",
        "print(\"discount factor: \", env.gamma)\n",
        "print(\"\")\n",
        "\n",
        "# Usefult methods\n",
        "state = env.reset() # get initial state\n",
        "print(\"initial state: \", state)\n",
        "print(\"reward at (s=1, a=3,s'=2): \", env.reward_func(1,3,2))\n",
        "print(\"\")\n",
        "\n",
        "# A random policy\n",
        "policy = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(\"random policy = \", policy)\n",
        "\n",
        "# Interacting with the environment\n",
        "print(\"(s, a, s', r):\")\n",
        "for time in range(4):\n",
        "    action = policy[state]\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    print(state, action, next_state, reward)\n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "print(\"\")\n",
        "print(env.R.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AUlNvT3cOPfx"
      },
      "source": [
        "## Question 1: Value iteration\n",
        "1. Write a function applying the optimal Bellman operator on a provided Q function: $Q_1 = LQ_0, \\; Q_0\\in \\mathbb{R}^{S\\times A}$\n",
        "2. Write a function implementing Value Iteration (VI) with $\\infty$-norm stopping condition (reuse function implemented in 1)\n",
        "3. Evaluate the convergence of your estimate, i.e., plot the value $\\|Q_n - Q^\\star\\|_{\\infty} = \\max_{s,a} |Q_n(s,a) - Q^\\star(s,a)|$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R8TLRx6MOPfy",
        "colab": {}
      },
      "source": [
        "# --------------\n",
        "# Point 1\n",
        "# --------------\n",
        "def bellman_operator(Q0, Ns, Na, R, P, gamma):\n",
        "    \n",
        "    # Computing Q1 = LQ0 #\n",
        "    Q1 = np.zeros((Ns,Na))\n",
        "    greedy_policy = np.zeros(Ns)\n",
        "    for s in range(Ns):\n",
        "        for a in range(Na):\n",
        "            Q1[s,a] = np.dot(P[s,a,:], R[s,a,:]) + gamma * np.dot(P[s,a,:], np.max(Q0, axis = 1))\n",
        "            \n",
        "    # Computing the greedy policy #\n",
        "    for s in range(Ns):\n",
        "        greedy_policy[s] = np.argmax(Q1[s,:])\n",
        "        \n",
        "    return Q1, greedy_policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jj65cQk5OPf0",
        "colab": {}
      },
      "source": [
        "# --------------\n",
        "# Point 2\n",
        "# --------------\n",
        "def value_iteration(Q0, env, epsilon=1e-6):\n",
        "    gamma = env.gamma\n",
        "    Q_history = [Q0]\n",
        "    err = epsilon + 1\n",
        "    while err >= epsilon:\n",
        "        Q0, greedy_policy = bellman_operator(Q0, env.Ns, env.Na, env.R, env.P, gamma)\n",
        "        err = np.max(np.abs(Q0 - Q_history[-1]))\n",
        "        Q_history.append(Q0)\n",
        "\n",
        "    return Q0, greedy_policy, Q_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bftNXoV1_KUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --------------\n",
        "# Point 3\n",
        "# --------------\n",
        "\n",
        "\n",
        "with open(\"./mvarl_hands_on/data/Q_opts.json\", \"r\") as fp:\n",
        "    Qopts = json.load(fp)\n",
        "Qstar = Qopts[\"{}_{}\".format(type(env).__name__,env.gamma)]\n",
        "\n",
        "Q0 = np.zeros((env.Ns,env.Na))\n",
        "Q, greedy_policy, Q_history = value_iteration(Q0, env, epsilon = 1e-8)\n",
        "\n",
        "norm_values = [np.max(np.abs(Q_history[i] - Qstar)) for i in range(len(Q_history))]\n",
        "\n",
        "plt.plot(norm_values)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Error')\n",
        "plt.title(\"Q-learning: Convergence of Q\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "azfXmRzZOPf4"
      },
      "source": [
        "## Question 2: Q learning\n",
        "Q learning is a model-free algorithm for estimating the optimal Q-function online.\n",
        "It is an off-policy algorithm since the samples are collected with a policy that is (potentially) not the one associated to the estimated Q-function.\n",
        "\n",
        "1. Implement Q learning with $\\epsilon$-greedy exploration.\n",
        "  - Plot the error in Q-functions over iterations\n",
        "  - Plot the sum of rewards as a function of iteration\n",
        "\n",
        "\n",
        "$\\epsilon$-greedy policy:\n",
        "$$\n",
        "\\pi(s) = \\begin{cases}\n",
        "\\max_a Q(s,a) & \\text{w.p.} \\epsilon\\\\\n",
        "\\text{random action} & \\text{w.p.} 1- \\epsilon\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAre6ZVfAJcJ",
        "colab_type": "text"
      },
      "source": [
        "#Notes\n",
        "\n",
        "The learning rate $\\alpha_{s,a}(t)$ , to ensure convergence, should satistify the Robbins-Monro conditions, *i.e.*\n",
        "$$\n",
        "\\sum_{t} \\alpha_{s,a}(t) = \\infty, \\quad \\sum_{t} \\alpha_{s,a}(t)^2 < \\infty.\n",
        "$$\n",
        "In our algorithm, we coud have set, for instance, $\\alpha_{s,a}(t) = 1/(N_t(s,a))^{1/2 + \\varepsilon}$ with $\\varepsilon > 0$ and $N_t(s,a)$ the number of times the state-action $(s,a)$ has been visited at time $t$. Nonetheless, we have decided to put \n",
        "$$\n",
        "\\alpha_{s,a}(t) := \\frac{1}{\\log( e + \\log(1 + N_t(s,a))} \\quad (1).\n",
        "$$\n",
        "This learning rate **does not** satisfy Robbins-Monro conditions, so technically, nothing ensure convergence ! That being said, when we tested our code with a learning rate that did verify those conditions, we saw that the convergence was very slow ! This might be explained by the fact that we started quite far away from the optimal solution. The learning rate (1) has the advantage of decreasing very slowly, so that we take enough big steps at the beginning of the learning process, and therefore we experimentally observe a better approximation of the true $Q^\\star$. The drawback is that, near the optimum, we do observe fluctuations (*i.e.* the gap, after decreasing, rebounces)  for the learning rate does not decay quickly enough. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_t3WqIt-OPf5",
        "colab": {}
      },
      "source": [
        "# ---------------------------\n",
        "# Q-Learning\n",
        "# ---------------------------\n",
        "# suggested interface\n",
        "# you can change it!\n",
        "\n",
        "class QLearning:\n",
        "    \"\"\"\n",
        "    Q learning with epsilon-greedy exploration\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        self.env = env \n",
        "        self.Q = np.zeros((env.Ns, env.Na))\n",
        "        self.count = np.zeros((env.Ns, env.Na))\n",
        "        \n",
        "    def learning_rate(self, state, action):\n",
        "        return 1/np.log(np.e + np.log(1 + self.count[state, action]))\n",
        "        \n",
        "    def sample_action(self, state, greedy):\n",
        "        h = np.random.rand()\n",
        "        if h < greedy:\n",
        "            return np.argmax(self.Q[state, :])\n",
        "        else:\n",
        "            return np.random.randint(env.Na)\n",
        "    \n",
        "    def update(self, state, action, next_state, reward):\n",
        "        self.count[state, action] += 1\n",
        "        lr = self.learning_rate(state, action)\n",
        "        self.Q[state,action] = (1 - lr) * self.Q[state, action] \\\n",
        "                            + lr * (reward + self.env.gamma * np.max(self.Q[next_state,:]))\n",
        "        \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CKTc5nWIOPf6",
        "colab": {}
      },
      "source": [
        "# --------------\n",
        "# Point 1\n",
        "# --------------\n",
        "# Number of Q learning steps\n",
        "max_steps = int(1e5)  \n",
        "# max_steps = 10\n",
        "\n",
        "Q0 = np.zeros((env.Ns, env.Na))\n",
        "# Use the previous code to verify the correctness of q learning\n",
        "Q_opt, pi_opt, Q_hist = value_iteration(Q0, env, epsilon=1e-8)\n",
        "\n",
        "\n",
        "# main algorithmic loop\n",
        "env.reset()\n",
        "state = env.state\n",
        "ql = QLearning(env)\n",
        "greedy = 0.2\n",
        "norm_values = []\n",
        "cum_rewards = [0.]\n",
        "t = 0\n",
        "\n",
        "while t < max_steps:\n",
        "    action = ql.sample_action(state, greedy)\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    ql.update(state, action, observation, reward)\n",
        "    state = observation\n",
        "    norm_values.append(np.abs(ql.Q - Q_opt).mean())\n",
        "    cum_rewards.append(env.gamma * cum_rewards[-1] + reward)\n",
        "    t = t + 1\n",
        "    \n",
        "print(env.render())\n",
        "print(\"optimal policy: \", pi_opt)\n",
        "greedy_policy = np.argmax(ql.Q, axis=1)\n",
        "print(\"est policy:\", greedy_policy)\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(10, 4))\n",
        "plt.subplot(121)\n",
        "\n",
        "plt.semilogy(norm_values)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Error')\n",
        "plt.title(\"Q-learning: Convergence of Q\")\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.plot(cum_rewards, c='r')\n",
        "plt.xlabel('Iteration')\n",
        "plt.title(\"Cumulative reward\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pBttV9L_KU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}